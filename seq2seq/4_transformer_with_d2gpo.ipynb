{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_with_d2gpo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJzCDhlj5cWp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "70427a0c-b666-40a7-ad9f-e08c70016f32"
      },
      "source": [
        "!pip install keras-transformer\n",
        "import h5py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "data_dir = \"drive/My Drive/CS4995 Deep Learning/Competitions_Project/Final_Project/data/3000/new/\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-transformer in /usr/local/lib/python3.6/dist-packages (0.33.0)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer) (0.6.0)\n",
            "Requirement already satisfied: keras-layer-normalization>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer) (0.14.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-transformer) (2.3.1)\n",
            "Requirement already satisfied: keras-embed-sim>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer) (0.7.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-transformer) (1.18.3)\n",
            "Requirement already satisfied: keras-pos-embd>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer) (0.11.0)\n",
            "Requirement already satisfied: keras-multi-head>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer) (0.22.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-transformer) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-transformer) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-transformer) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-transformer) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-transformer) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-transformer) (1.0.8)\n",
            "Requirement already satisfied: keras-self-attention==0.41.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.22.0->keras-transformer) (0.41.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOPkU3lB5iDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "pickle_in = open(data_dir+'data_250.pickle', 'rb')\n",
        "data = pickle.load(pickle_in)\n",
        "pickle_in.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUM1gV81GRl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(data_dir+'stocks_emb.w2vec', \"r\") as f:\n",
        "    lines = f.read().split('\\n')  #embedded vectors reading\n",
        "f1 = [item.split(' ') for item in lines[1:]]\n",
        "word2vec = {item[0]: np.array(list(map(float, item[1:]))) for item in f1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odoxXLe940sy",
        "colab_type": "code",
        "outputId": "8a1e5de1-f5a1-4c01-dd67-6b09a42bdfe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras_transformer import get_model, decode\n",
        "\n",
        "source_tokens = [item[0].split(',') for item in data]  #source_tokens[0] = ['AAAA', 'BBBB',...,'ZZZZ']\n",
        "target_tokens =  [item[1].split(',') for item in data]\n",
        "\n",
        "# Generate dictionaries\n",
        "def build_token_dict(token_list):\n",
        "    token_dict = {\n",
        "        '<PAD>': 0,\n",
        "        '<START>': 1,\n",
        "        '<END>': 2,\n",
        "    }\n",
        "    for tokens in token_list:\n",
        "        for token in tokens:\n",
        "            if token not in token_dict:\n",
        "                token_dict[token] = len(token_dict)\n",
        "    return token_dict    #input is a two-dimensional matrix, tokens is the ['AAAA', 'BBBB',...,'ZZZZ'], token is 'AAAA'\n",
        "\n",
        "source_token_dict = build_token_dict(source_tokens)\n",
        "target_token_dict = build_token_dict(target_tokens)\n",
        "target_token_dict_inv = {v: k for k, v in target_token_dict.items()}  #we need the rev as {1:'AAAA', 2:'BBBB', ...}\n",
        "\n",
        "#record the corresponding index of the target_token_dict\n",
        "tensor_dict = {}\n",
        "for key in word2vec.keys():\n",
        "    tensor_dict[key] = len(tensor_dict)\n",
        "\n",
        "tensor_list = []\n",
        "\n",
        "for key in target_token_dict.keys():\n",
        "    try:\n",
        "        tensor_list.append(tensor_dict[key])\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "with h5py.File(data_dir + \"stocks_emb.gaussian_1_0_0.h5\", 'r') as f:\n",
        "    dist_data = np.array(f.get('weights'))\n",
        "print(len(dist_data))\n",
        "dist_data = dist_data[tensor_list, tensor_list]\n",
        "print(len(dist_data))\n",
        "\n",
        "dist_data = tf.convert_to_tensor(dist_data)\n",
        "\n",
        "len(tensor_list)\n",
        "# Add special tokens\n",
        "encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens] #it's input of encoder\n",
        "decode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens] #it's input of decoder\n",
        "output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens] #it's the ground truth of decoder\n",
        "\n",
        "# Padding\n",
        "source_max_len = max(map(len, encode_tokens))  #the size of source sequence\n",
        "target_max_len = max(map(len, decode_tokens))   #the size of target sequence\n",
        "\n",
        "encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens] #tokens mean the sequence, padding enough <PAD> here!\n",
        "decode_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in decode_tokens]\n",
        "output_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in output_tokens]\n",
        "\n",
        "encode_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encode_tokens] #map token in tokens into the number (sparse)\n",
        "decode_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decode_tokens]\n",
        "decode_output = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens] #why list of list? \n",
        "\n",
        "# Build & fit model\n",
        "model = get_model(\n",
        "    token_num=max(len(source_token_dict), len(target_token_dict)),\n",
        "    embed_dim=32,\n",
        "    encoder_num=2,\n",
        "    decoder_num=2,\n",
        "    head_num=4,\n",
        "    hidden_dim=128,\n",
        "    dropout_rate=0.05,\n",
        "    use_same_embed=False,  # Use different embeddings for different languages\n",
        ")\n",
        "\n",
        "#handcrafted loss term with d2gpo\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras.losses import kullback_leibler_divergence, sparse_categorical_crossentropy\n",
        "import numpy as np\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "#self handicrafted d2gpo loss term\n",
        "def wrapper_loss(topo_dict, lmbda):\n",
        "    def model_loss(y_true, y_pred):\n",
        "        ce_loss = sparse_categorical_crossentropy(y_true, y_pred)\n",
        "        y_true_idx = tf.cast(y_true, tf.int64)\n",
        "        kl_loss = kullback_leibler_divergence(tf.cast(tf.gather(dist_data, y_true_idx), tf.float32), y_pred)\n",
        "        return (1-lmbda)*ce_loss + lmbda*kl_loss\n",
        "    return model_loss\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "3209\n",
            "1268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLI_yZmCDpOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimizer and compile\n",
        "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "model.compile('adam', wrapper_loss(dist_data, 0.95))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dXr4w7jDmYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        },
        "outputId": "140a61eb-b3ea-4dac-dd18-b16a4807cf91"
      },
      "source": [
        "#model fitting\n",
        "train_start = 0\n",
        "train_end = 200\n",
        "model.fit(\n",
        "    x=[np.array(encode_input )[train_start: train_end], np.array(decode_input)[train_start: train_end]],\n",
        "    y=np.array(decode_output)[train_start: train_end],\n",
        "    epochs=16,\n",
        "    batch_size=1,\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "200/200 [==============================] - 8s 38ms/step - loss: -5.3131e-05\n",
            "Epoch 2/16\n",
            "200/200 [==============================] - 5s 23ms/step - loss: -0.0096\n",
            "Epoch 3/16\n",
            "200/200 [==============================] - 5s 23ms/step - loss: -0.0111\n",
            "Epoch 4/16\n",
            "200/200 [==============================] - 5s 23ms/step - loss: -0.0119\n",
            "Epoch 5/16\n",
            "200/200 [==============================] - 5s 23ms/step - loss: -0.0127\n",
            "Epoch 6/16\n",
            "200/200 [==============================] - 5s 23ms/step - loss: -0.0135\n",
            "Epoch 7/16\n",
            "200/200 [==============================] - 5s 23ms/step - loss: -0.0141\n",
            "Epoch 8/16\n",
            "200/200 [==============================] - 5s 23ms/step - loss: -0.0147\n",
            "Epoch 9/16\n",
            "200/200 [==============================] - 5s 23ms/step - loss: -0.0153\n",
            "Epoch 10/16\n",
            "200/200 [==============================] - 5s 23ms/step - loss: -0.0159\n",
            "Epoch 11/16\n",
            "200/200 [==============================] - 5s 23ms/step - loss: -0.0163\n",
            "Epoch 12/16\n",
            "200/200 [==============================] - 4s 22ms/step - loss: -0.0168\n",
            "Epoch 13/16\n",
            "200/200 [==============================] - 4s 22ms/step - loss: -0.0174\n",
            "Epoch 14/16\n",
            "200/200 [==============================] - 4s 22ms/step - loss: -0.0181\n",
            "Epoch 15/16\n",
            "200/200 [==============================] - 4s 22ms/step - loss: -0.0189\n",
            "Epoch 16/16\n",
            "200/200 [==============================] - 5s 23ms/step - loss: -0.0198\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f8055829ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDB6N3pQ45N9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the function that calculate our handcrafted metric\n",
        "def accuracy(i):\n",
        "    decoded = decode(\n",
        "        model,\n",
        "        encode_input[i],\n",
        "        start_token=target_token_dict['<START>'],\n",
        "        end_token=target_token_dict['<END>'],\n",
        "        pad_token=target_token_dict['<PAD>'],\n",
        "        top_k=1,\n",
        "        temperature=1,\n",
        "    )\n",
        "    predicted = (','.join(map(lambda x: target_token_dict_inv[x], decoded[1:-1]))).split(',')\n",
        "    target = set(target_tokens[i])\n",
        "    accuracy = 0\n",
        "    for item in predicted:\n",
        "        if item in target:\n",
        "            accuracy += 1\n",
        "    return accuracy / len(predicted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO8o0-Rj9Aqj",
        "colab_type": "code",
        "outputId": "d725cfaf-98f9-4b21-dd10-ac850bdfea0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#calculate the average accuracy of the validation set\n",
        "import tqdm\n",
        "np.mean([accuracy(i) for i in tqdm.tqdm(range(0, 250))]) \n",
        "\n",
        "\n",
        "#on all 250 data\n",
        "#0.011 | lmbda = 1.00\n",
        "#0.112 | lmbda = 0.95\n",
        "#0.117 | lmbda = 0.80\n",
        "#0.108 | lmbda = 0.30\n",
        "#0.094 | lmbda = 0.20\n",
        "#0.088 | lmbda = 0.10\n",
        "#0.050 | lmbda = 0.00\n",
        "#"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 250/250 [00:29<00:00,  8.34it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0994"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxkyExj4CUsf",
        "colab_type": "code",
        "outputId": "7209b0aa-50e8-458a-e3fd-cb2f5be31c3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        }
      },
      "source": [
        "#example of inferences\n",
        "decoded = decode(\n",
        "                            model,\n",
        "                            encode_input[3],\n",
        "                            start_token=target_token_dict['<START>'],\n",
        "                            end_token=target_token_dict['<END>'],\n",
        "                            pad_token=target_token_dict['<PAD>'],\n",
        "                            top_k=3,\n",
        "                            temperature=1)\n",
        "(','.join(map(lambda x: target_token_dict_inv[x], decoded[1:-1]))).split(',')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['TBLTW',\n",
              " 'PAVMW',\n",
              " 'PAVMW',\n",
              " 'TBLTW',\n",
              " 'RBZ',\n",
              " 'OCGN',\n",
              " 'RBZ',\n",
              " 'AMRS',\n",
              " 'AMRS',\n",
              " 'SNSS',\n",
              " 'PHUN',\n",
              " 'MRAM',\n",
              " 'IPDN',\n",
              " 'PHUN',\n",
              " 'SNSS',\n",
              " 'SNSS',\n",
              " 'MICT',\n",
              " 'RBZ',\n",
              " 'ENLV',\n",
              " 'RETO',\n",
              " 'FRSX',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " 'TMDI',\n",
              " 'TBLTW',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " 'CFMS',\n",
              " 'RBZ',\n",
              " 'RETO',\n",
              " 'ICON',\n",
              " '<PAD>',\n",
              " 'SES',\n",
              " 'PHUN',\n",
              " 'CFMS',\n",
              " 'IPWR',\n",
              " 'DGLY',\n",
              " 'IFMK',\n",
              " 'SLRX',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " '<PAD>',\n",
              " 'BOXL',\n",
              " 'PHUN',\n",
              " 'CFMS',\n",
              " 'CFMS',\n",
              " 'CLDX',\n",
              " 'ICON',\n",
              " 'CFMS',\n",
              " 'YVR',\n",
              " 'YVR',\n",
              " 'NAKD',\n",
              " 'CETXP']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    }
  ]
}