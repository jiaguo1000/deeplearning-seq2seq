{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_without_d2gpo.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXAS6JwBJSJ9",
        "colab_type": "code",
        "outputId": "a3bdf559-bdc5-401b-92c4-b6f809acc178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "!pip install keras_transformer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras_transformer import get_model, decode\n",
        "data_dir = \"drive/My Drive/CS4995 Deep Learning/Competitions_Project/Final_Project/data/3000/new/\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras_transformer in /usr/local/lib/python3.6/dist-packages (0.33.0)\n",
            "Requirement already satisfied: keras-embed-sim>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from keras_transformer) (0.7.0)\n",
            "Requirement already satisfied: keras-layer-normalization>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from keras_transformer) (0.14.0)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from keras_transformer) (0.6.0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_transformer) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras_transformer) (1.18.3)\n",
            "Requirement already satisfied: keras-multi-head>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from keras_transformer) (0.22.0)\n",
            "Requirement already satisfied: keras-pos-embd>=0.10.0 in /usr/local/lib/python3.6/dist-packages (from keras_transformer) (0.11.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_transformer) (1.0.8)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_transformer) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras_transformer) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_transformer) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_transformer) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_transformer) (1.12.0)\n",
            "Requirement already satisfied: keras-self-attention==0.41.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.22.0->keras_transformer) (0.41.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qehpN2dnJgyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load the pairs list we have dumped with pickle package\n",
        "import pickle\n",
        "pickle_in = open(data_dir+\"dict.pickle\",\"rb\")\n",
        "data = pickle.load(pickle_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfR_agPaJZrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras_transformer import get_model, decode\n",
        "\n",
        "source_tokens = [item[0].split(',') for item in data]  #source_tokens[0] = ['AAAA', 'BBBB',...,'ZZZZ']\n",
        "target_tokens =  [item[1].split(',') for item in data]  #target_tokens[0] = ['AA', 'BB', ..., 'ZZ']\n",
        "\n",
        "# define a function to generate token dictionary\n",
        "def build_token_dict(token_list):\n",
        "    token_dict = {\n",
        "        '<PAD>': 0,\n",
        "        '<START>': 1,\n",
        "        '<END>': 2,\n",
        "    }\n",
        "    for tokens in token_list:\n",
        "        for token in tokens:\n",
        "            if token not in token_dict:\n",
        "                token_dict[token] = len(token_dict)\n",
        "    return token_dict    #input is a two-dimensional matrix, tokens is the ['AAAA', 'BBBB',...,'ZZZZ'], token is 'AAAA'\n",
        "\n",
        "source_token_dict = build_token_dict(source_tokens)\n",
        "target_token_dict = build_token_dict(target_tokens)\n",
        "target_token_dict_inv = {v: k for k, v in target_token_dict.items()}  #we need the rev as {1:'AAAA', 2:'BBBB', ...}, which is for the sample inference\n",
        " \n",
        " \n",
        "# add special tokens to every sequence\n",
        "encode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in source_tokens] #it's input of encoder\n",
        "decode_tokens = [['<START>'] + tokens + ['<END>'] for tokens in target_tokens] #it's input of decoder\n",
        "output_tokens = [tokens + ['<END>', '<PAD>'] for tokens in target_tokens] #it's the ground truth of decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwMmBCOu4Rpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# padding and input preparation\n",
        "source_max_len = max(map(len, encode_tokens))  #the size of source sequence, which is fixed\n",
        "target_max_len = max(map(len, decode_tokens))   #the size of target sequence, which is fixed\n",
        "\n",
        "encode_tokens = [tokens + ['<PAD>'] * (source_max_len - len(tokens)) for tokens in encode_tokens] #tokens mean the sequence, padding enough <PAD> here!\n",
        "decode_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in decode_tokens]\n",
        "output_tokens = [tokens + ['<PAD>'] * (target_max_len - len(tokens)) for tokens in output_tokens]\n",
        "\n",
        "encode_input = [list(map(lambda x: source_token_dict[x], tokens)) for tokens in encode_tokens] #map token in tokens into the number (sparse)\n",
        "decode_input = [list(map(lambda x: target_token_dict[x], tokens)) for tokens in decode_tokens]\n",
        "decode_output = [list(map(lambda x: [target_token_dict[x]], tokens)) for tokens in output_tokens] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okBYGoy6-7oa",
        "colab_type": "code",
        "outputId": "fbc53378-68a7-40ed-9fb8-9ddc1f8ed1e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# build our transformer model with the same embedding space\n",
        "model = get_model(\n",
        "    token_num=max(len(source_token_dict), len(target_token_dict)),\n",
        "    embed_dim=32,\n",
        "    encoder_num=2,\n",
        "    decoder_num=2,\n",
        "    head_num=4,\n",
        "    hidden_dim=128,\n",
        "    dropout_rate=0.05,\n",
        "    use_same_embed=True,  # Use different embeddings for different languages\n",
        ")\n",
        "\n",
        "#without d2gpo\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "#optimizer and compile\n",
        "optimizer = Adam(lr=0.0001)\n",
        "model.compile('adam', 'sparse_categorical_crossentropy')\n",
        "\n",
        "#model fitting\n",
        "train_start = 3750\n",
        "train_end = 4850\n",
        "model.fit(\n",
        "    x=[np.array(encode_input)[train_start:train_end], np.array(decode_input)[train_start:train_end]],\n",
        "    y=np.array(decode_output)[train_start:train_end],\n",
        "    epochs=128,\n",
        "    batch_size=16,\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/128\n",
            "1100/1100 [==============================] - 6s 6ms/step - loss: 7.3154\n",
            "Epoch 2/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.7262\n",
            "Epoch 3/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.4685\n",
            "Epoch 4/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.3597\n",
            "Epoch 5/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.3143\n",
            "Epoch 6/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.2887\n",
            "Epoch 7/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.2655\n",
            "Epoch 8/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.2432\n",
            "Epoch 9/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.2155\n",
            "Epoch 10/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.1867\n",
            "Epoch 11/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.1546\n",
            "Epoch 12/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.1227\n",
            "Epoch 13/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.0926\n",
            "Epoch 14/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.0562\n",
            "Epoch 15/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 6.0221\n",
            "Epoch 16/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.9917\n",
            "Epoch 17/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.9664\n",
            "Epoch 18/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.9324\n",
            "Epoch 19/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.9021\n",
            "Epoch 20/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.8728\n",
            "Epoch 21/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.8457\n",
            "Epoch 22/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.8153\n",
            "Epoch 23/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.7832\n",
            "Epoch 24/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.7519\n",
            "Epoch 25/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.7173\n",
            "Epoch 26/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.6817\n",
            "Epoch 27/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.6466\n",
            "Epoch 28/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.6097\n",
            "Epoch 29/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.5738\n",
            "Epoch 30/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.5347\n",
            "Epoch 31/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.4906\n",
            "Epoch 32/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.4500\n",
            "Epoch 33/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.4066\n",
            "Epoch 34/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.3746\n",
            "Epoch 35/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.3317\n",
            "Epoch 36/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.2912\n",
            "Epoch 37/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.2507\n",
            "Epoch 38/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.2041\n",
            "Epoch 39/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.1701\n",
            "Epoch 40/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.1255\n",
            "Epoch 41/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.0886\n",
            "Epoch 42/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.0482\n",
            "Epoch 43/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 5.0086\n",
            "Epoch 44/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.9781\n",
            "Epoch 45/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.9379\n",
            "Epoch 46/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.9113\n",
            "Epoch 47/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.8705\n",
            "Epoch 48/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.8354\n",
            "Epoch 49/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.8014\n",
            "Epoch 50/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.7774\n",
            "Epoch 51/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.7419\n",
            "Epoch 52/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.7190\n",
            "Epoch 53/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.6803\n",
            "Epoch 54/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.6534\n",
            "Epoch 55/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.6315\n",
            "Epoch 56/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.6027\n",
            "Epoch 57/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.5780\n",
            "Epoch 58/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.5458\n",
            "Epoch 59/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.5130\n",
            "Epoch 60/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.4914\n",
            "Epoch 61/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.4693\n",
            "Epoch 62/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.4417\n",
            "Epoch 63/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.4163\n",
            "Epoch 64/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.3962\n",
            "Epoch 65/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.3795\n",
            "Epoch 66/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.3571\n",
            "Epoch 67/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.3273\n",
            "Epoch 68/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.3153\n",
            "Epoch 69/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.2917\n",
            "Epoch 70/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.2645\n",
            "Epoch 71/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.2480\n",
            "Epoch 72/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.2356\n",
            "Epoch 73/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.2159\n",
            "Epoch 74/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.2060\n",
            "Epoch 75/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.1731\n",
            "Epoch 76/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.1627\n",
            "Epoch 77/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.1473\n",
            "Epoch 78/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.1334\n",
            "Epoch 79/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.1082\n",
            "Epoch 80/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.0876\n",
            "Epoch 81/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.0777\n",
            "Epoch 82/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.0685\n",
            "Epoch 83/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.0623\n",
            "Epoch 84/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.0345\n",
            "Epoch 85/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.0106\n",
            "Epoch 86/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 4.0163\n",
            "Epoch 87/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.9978\n",
            "Epoch 88/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.9785\n",
            "Epoch 89/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.9678\n",
            "Epoch 90/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.9431\n",
            "Epoch 91/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.9301\n",
            "Epoch 92/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.9265\n",
            "Epoch 93/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.9100\n",
            "Epoch 94/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.8929\n",
            "Epoch 95/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.8842\n",
            "Epoch 96/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.8646\n",
            "Epoch 97/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.8538\n",
            "Epoch 98/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.8465\n",
            "Epoch 99/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.8330\n",
            "Epoch 100/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.8185\n",
            "Epoch 101/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.8079\n",
            "Epoch 102/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.7967\n",
            "Epoch 103/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.7983\n",
            "Epoch 104/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.7656\n",
            "Epoch 105/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.7708\n",
            "Epoch 106/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.7541\n",
            "Epoch 107/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.7374\n",
            "Epoch 108/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.7285\n",
            "Epoch 109/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.7189\n",
            "Epoch 110/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.7156\n",
            "Epoch 111/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.6934\n",
            "Epoch 112/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.6973\n",
            "Epoch 113/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.6901\n",
            "Epoch 114/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.6727\n",
            "Epoch 115/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.6686\n",
            "Epoch 116/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.6568\n",
            "Epoch 117/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.6343\n",
            "Epoch 118/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.6248\n",
            "Epoch 119/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.6401\n",
            "Epoch 120/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.6278\n",
            "Epoch 121/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.6049\n",
            "Epoch 122/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.6068\n",
            "Epoch 123/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.5871\n",
            "Epoch 124/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.5769\n",
            "Epoch 125/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.5693\n",
            "Epoch 126/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.5589\n",
            "Epoch 127/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.5603\n",
            "Epoch 128/128\n",
            "1100/1100 [==============================] - 2s 2ms/step - loss: 3.5475\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f82521de6d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqlQqTvNJaKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define the function to calculate the metric accuracy\n",
        "def accuracy(i, k, temp):\n",
        "    decoded = decode(\n",
        "        model,\n",
        "        encode_input[i],\n",
        "        start_token=target_token_dict['<START>'],\n",
        "        end_token=target_token_dict['<END>'],\n",
        "        pad_token=target_token_dict['<PAD>'],\n",
        "        top_k=3,\n",
        "        temperature=1,\n",
        "    )  \n",
        "    predicted = (','.join(map(lambda x: target_token_dict_inv[x], decoded[1:-1]))).split(',')\n",
        "    target = set(target_tokens[i])\n",
        "    accuracy = 0\n",
        "    for item in predicted:\n",
        "        if item in target:\n",
        "            accuracy += 1\n",
        "    return accuracy / len(predicted)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rosynKN1Je6i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "e2153868-e00c-472d-8464-0bfc7ecad479"
      },
      "source": [
        "#sensitivity analysis\n",
        "import tqdm\n",
        "topk_list = [1, 3, 5, 7]\n",
        "temperature_list = [0.4, 1, 4, 10]\n",
        "\n",
        "#analysis for topk\n",
        "res_topk = []\n",
        "for k in topk_list:\n",
        "    res_topk.append([accuracy(i, k, 1) for i in tqdm.tqdm(range(train_end, 5040))])\n",
        "\n",
        "#analysis for temperature\n",
        "res_temp = []\n",
        "for t in temperature_list:\n",
        "    res_temp.append([accuracy(i, 3, t) for i in tqdm.tqdm(range(train_end, 5040))])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 190/190 [00:40<00:00,  4.70it/s]\n",
            "100%|██████████| 190/190 [00:39<00:00,  4.81it/s]\n",
            "100%|██████████| 190/190 [00:40<00:00,  4.72it/s]\n",
            "100%|██████████| 190/190 [00:38<00:00,  4.97it/s]\n",
            "100%|██████████| 190/190 [00:40<00:00,  4.74it/s]\n",
            "100%|██████████| 190/190 [00:37<00:00,  5.10it/s]\n",
            "100%|██████████| 190/190 [00:40<00:00,  4.69it/s]\n",
            "100%|██████████| 190/190 [00:37<00:00,  5.05it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_k1PUFoLlV_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "21ad450f-8813-42e1-99b1-5565bf7f69ad"
      },
      "source": [
        "#model inference part\n",
        "decoded = decode(\n",
        "        model,\n",
        "        encode_input[11],\n",
        "        start_token=target_token_dict['<START>'],\n",
        "        end_token=target_token_dict['<END>'],\n",
        "        pad_token=target_token_dict['<PAD>'],\n",
        "        top_k=3,\n",
        "        temperature=1,\n",
        "    )\n",
        "predicted = (','.join(map(lambda x: target_token_dict_inv[x], decoded[1:-1]))).split(',')\n",
        "target = target_tokens[11]\n",
        "for i in zip(predicted, target):\n",
        "    print(i)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('CSSE', 'CDMO')\n",
            "('BCOV', 'PCYO')\n",
            "('IIN', 'OCC')\n",
            "('XSPA', 'CLWT')\n",
            "('RLMD', 'BOOM')\n",
            "('NTGN', 'WWR')\n",
            "('NTGN', 'CTG')\n",
            "('GERN', 'SNGX')\n",
            "('SCKT', 'GERN')\n",
            "('AOBC', 'CVV')\n",
            "('AVID', 'PLUG')\n",
            "('BPTH', 'GEOS')\n",
            "('MTCH', 'SIGA')\n",
            "('PDLI', 'OSIS')\n",
            "('BNGO', 'CYAN')\n",
            "('LX', 'FTEK')\n",
            "('BTAI', 'IVAC')\n",
            "('NMIH', 'AVDL')\n",
            "('KNSA', 'ABMD')\n",
            "('TTMI', 'TGA')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Avg9-pGL3fl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}